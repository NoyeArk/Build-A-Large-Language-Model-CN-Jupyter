{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81820166",
   "metadata": {},
   "source": [
    "# 4. 从零开始实现一个用于文本生成的GPT模型\n",
    "\n",
    "本章涵盖以下内容：\n",
    "\n",
    "+ **编写一个类 GPT 的大语言模型（LLM），可以训练其生成类人文本（指的是由人工智能模型生成的文本，这些文本在语言表达、语法结构、情感表达等方面与人类自然书写的文本非常相似）**\n",
    "+ **对网络层的激活值进行归一化，以稳定神经网络的训练过程**\n",
    "+ **在深度神经网络中添加快捷连接，以更高效地训练模型**\n",
    "+ **通过实现 Transformer 模块来构建不同规模的 GPT 模型**\n",
    "+ **计算 GPT 模型的参数数量和存储需求**\n",
    "\n",
    "-----\n",
    "\n",
    "- [4.1 实现 LLM 的架构](#41-实现-llm-的架构)\n",
    "- [4.2 使用层归一化对激活值进行标准化](#42-使用层归一化对激活值进行标准化)\n",
    "- [4.3 实现带有 GELU 激活函数的前馈神经网络](#43-实现带有-gelu-激活函数的前馈神经网络)\n",
    "- [4.4 添加快捷连接](#44-添加快捷连接)\n",
    "- [4.5 在 Transformer 模块中连接注意力层与线性层](#45-在-transformer-模块中连接注意力层与线性层)\n",
    "- [4.6 实现 GPT 模型](#46-实现-gpt-模型)\n",
    "- [4.7 生成文本](#47-生成文本)\n",
    "- [4.8 本章摘要](#48-本章摘要)\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "在上一章中，我们学习并实现了多头注意力机制，这是大语言模型（LLM）的核心组件之一。本章将进一步实现 LLM 的其他组件，并将它们组装成一个与 GPT 类似结构的模型。我们将在下一章中训练该模型，以生成类人文本，具体过程如图 4.1 所示。\n",
    "\n",
    "<img src=\"../image/chapter4/figure4.1.png\" width=\"75%\" />\n",
    "\n",
    "大语言模型（LLM）架构（见图 4.1）由多个模块构成，我们将在本章中实现这些模块。接下来的内容，我们首先从整体视角介绍模型架构，然后详细讲解各个组件。\n",
    "\n",
    "\n",
    "\n",
    "## 4.1 实现 LLM 的架构\n",
    "\n",
    "LLM（如GPT，即生成式预训练 Transformer，Generative Pretrained Transformer）是一种大型深度神经网络架构，设计用于逐词（或逐 token）生成新文本。然而，尽管模型规模庞大，其结构却并没有想象中那么复杂，因为模型的许多组件是重复的（后文将对此展开说明）。图 4.2 展示了一个类 GPT 的 LLM 的整体视图，并突出了其主要组成部分。\n",
    "\n",
    "<img src=\"../image/chapter4/figure4.2.png\" width=\"75%\" />\n",
    "\n",
    "如图 4.2 所示，我们已经在之前的章节中讲解过几个模块，如输入的分词和嵌入，以及掩码多头注意力模块。本章的重点是实现 GPT 模型的核心结构（包括 Transformer 模块）。我们将在下一章对该模型进行训练，使其能够生成类人文本。\n",
    "\n",
    "在前几章中，为了简单起见，我们使用了较小的嵌入维度，确保概念和示例能够更方便地展示在一页内。而在本章中，我们将逐步扩展模型规模，达到小型 GPT-2 模型的大小（拥有1.24 亿参数量的最小版本）。该模型在 Radford 等人的论文《Language Models are Unsupervised Multitask Learners.》中有详细介绍。请注意，尽管最初的报告中提到参数量为 1.17 亿，但后来更正为 1.24 亿。\n",
    "\n",
    "第 6 章将重点介绍如何将预训练权重加载到我们的实现中，并将其调整为更大的 GPT-2 模型版本（包括 3.45 亿、7.62 亿和 15.42 亿参数量规模）。在深度学习和 GPT 等大语言模型的背景下，‘参数’一词指的是模型的可训练权重。这些权重本质上是模型的内部变量，在训练过程中不断调整和优化，以最小化特定的损失函数。这种优化使得模型能够从训练数据中学习。\n",
    "\n",
    "例如，在一个神经网络层中，其权重由一个 2,048 x 2,048 维的矩阵（或张量）表示，这个矩阵的每个元素都是一个参数。由于该矩阵有 2,048 行和 2,048 列，因此该层的总参数数量为 2,048 乘以 2,048，即 4,194,304 个参数。\n",
    "\n",
    "> [!NOTE]\n",
    ">\n",
    "> **GPT-2 与 GPT-3 的比较**\n",
    ">\n",
    "> 我们之所以关注 GPT-2，是因为 OpenAI 已公开了其预训练模型的权重，这些权重将在第 6 章中加载到我们的实现中。GPT-3 的模型架构基本上与 GPT-2 相同，只是将参数规模从 GPT-2 的 15 亿增加到了 1750 亿，同时在更多的数据上进行了训练。截至本文撰写时，GPT-3 的权重尚未公开。对于学习如何实现LLM，GPT-2 是更好的选择，因为它可以在单台笔记本电脑上运行，而 GPT-3 的训练和推理则需要 GPU 集群。根据 Lambda Labs 的估算，在单块 V100 数据中心 GPU 上训练 GPT-3 需要 355 年，而在消费级的 RTX 8000 GPU 上则需要 665 年。\n",
    "\n",
    "我们通过以下 Python 字典来定义小型 GPT-2 模型的配置，稍后将在代码示例中使用该配置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91b7530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dbe5b5",
   "metadata": {},
   "source": [
    "在 GPT_CONFIG_124M 字典中，我们使用简明的变量名，以保证清晰且避免代码行过长：\n",
    "\n",
    "+ `vocab_size`指的是第 2 章中 BPE 分词器使用的 50,257 个词汇的词表大小。\n",
    "+ `context_length`表示模型所能处理的最大输入 token 数（在第 2 章介绍位置嵌入时讨论过）。\n",
    "+ `emb_dim`表示嵌入维度，将每个 token 转换为 768 维的向量。\n",
    "+ `n_layers`指定模型中 Transformer 模块的层数，后续章节将对此详解。\n",
    "+ `drop_rate`表示 dropout 机制的强度（例如，0.1 表示丢弃 10% 的隐藏单元），用于防止过拟合，具体内容请回顾第 3 章。\n",
    "+ `qkv_bias` 参数决定是否在多头注意力的查询、键和值的线性层中加入偏置向量。我们最初会禁用该选项，以遵循现代大语言模型的标准，之后在第 6 章加载 OpenAI 预训练的 GPT-2 权重时再重新考虑该设置。\n",
    "\n",
    "使用上述配置，我们将从本章开始实现一个GPT占位架构（DummyGPTModel），如图4.3所示。这将为我们提供一个全局视图，了解所有组件如何组合在一起，以及在接下来的章节中需要编写哪些其他组件来组装完整的GPT模型架构。\n",
    "\n",
    "<img src=\"../image/chapter4/figure4.3.png\" width=\"75%\" />\n",
    "\n",
    "图 4.3 中显示的编号框说明了我们编写最终 GPT 架构所需理解的各个概念的顺序。我们将从第 1 步开始，这是一个我们称之为 DummyGPTModel 的 GPT 占位架构："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3d4256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 4.1 A placeholder GPT model architecture class\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])      #A\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])                       #B\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):                                       #C\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):                                                     #D\n",
    "        return x\n",
    "\n",
    "class DummyLayerNorm(nn.Module):                                              #E\n",
    "    def __init__(self, normalized_shape, eps=1e-5):                           #F\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "#A 为 TransformerBlock 设置占位符\n",
    "#B 为 LayerNorm 设置占位符\n",
    "#C 一个简单的占位类，后续将被真正的 TransformerBlock 替换\n",
    "#D 该模块无实际操作，仅原样返回输入\n",
    "#E 一个简单的占位类，后续将被真正的 DummyLayerNorm 替换\n",
    "#F 此处的参数仅用于模拟LayerNorm接口"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35b6b0",
   "metadata": {},
   "source": [
    "此代码中的 DummyGPTModel 类使用 PyTorch 内置的神经网络模块（nn.Module）定义了一个简化版的类 GPT 模型。该类包括 token 嵌入、位置嵌入、dropout、多个 Transformer 模块（DummyTransformerBlock）、最终的层归一化（DummyLayerNorm）以及线性输出层（out_head）。模型配置通过 Python 字典传入，稍后将传入我们之前创建的 GPT_CONFIG_124M 字典。\n",
    "\n",
    "`forward`方法定义了数据在模型中的流动方式：计算输入索引的 token 嵌入和位置嵌入，应用 dropout，通过 transformer block 处理数据，应用归一化，最后通过线性输出层生成 logits。\n",
    "\n",
    "上面的代码已经可以正常运行，不过需要先准备输入数据，在本节后面我们会看到运行效果。需要注意的是，目前代码中我们使用了 `DummyLayerNorm` 和 `DummyTransformerBlock` 作为 Transformer 模块和层归一化的占位符，实际的实现会在后续部分详细介绍。\n",
    "\n",
    "接下来，我们将准备输入数据并初始化一个新的 GPT 模型，以展示它的用法。基于第二章实现的分词器，图 4.4 展示了数据在 GPT 模型中流入和流出的整体流程。\n",
    "\n",
    "<img src=\"../image/chapter4/figure4.4.png\" width=\"75%\" />\n",
    "\n",
    "根据图 4.4 的步骤，我们使用第 2 章介绍的 tiktoken 分词器对包含两个文本的批量输入进行分词，以供 GPT 模型使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d7a9125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ee913",
   "metadata": {},
   "source": [
    "接下来，我们初始化一个拥有 1.24 亿参数的 DummyGPTModel 模型实例，并将分词后的数据批量输入到模型中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10dc1127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e5b794",
   "metadata": {},
   "source": [
    "输出的张量有两行，每行对应一段文本。每段文本包含 4 个 token，每个 token 是一个 50,257 维的向量，维度大小与分词器的词汇表相同。\n",
    "\n",
    "嵌入层的维度为 50,257，因为每个维度对应词汇表中的一个唯一 token。在之后的处理中，我们会将这些 50,257 维向量转换回 token ID，然后再解码成单词。\n",
    "\n",
    "在对 GPT 架构及其输入输出进行了大概介绍之后，接下来的章节中将编写各个占位模块的实现，首先从用真实的层归一化类替换之前代码中的 DummyLayerNorm 开始。\n",
    "\n",
    "---\n",
    "\n",
    "## 4.2 使用层归一化对激活值进行标准化\n",
    "\n",
    "在训练深度神经网络时，梯度消失或梯度爆炸问题有时会带来挑战。这些问题会导致训练过程不稳定，使得网络难以有效调整权重，也就是说，模型难以找到一组能最小化损失函数的参数。换句话说，模型很难从数据中学习到足够准确的模式，以支持其做出准确的预测或决策。（如果您对神经网络训练或梯度概念不熟悉，可参考附录 A 的 A.4 节《自动微分入门》。但要理解本书内容，不需要对梯度概念有深刻的理解。）\n",
    "\n",
    "> [!TIP]\n",
    ">\n",
    "> **个人思考：** 虽然对文本内容的理解并不需要深度掌握梯度的概念，但如果我们在学习过程中能习惯去发散，往往能帮助我们对所学知识理解的更深刻，下面我们就来聊一下梯度。\n",
    ">\n",
    "> 梯度本质上是一个**变化率**，描述了某个值（例如函数输出值）对另一个值（如输入变量）的变化趋势。简单来说，梯度告诉我们在当前位置上，朝哪个方向移动能让某个目标值增加或减少得更快。\n",
    ">\n",
    "> 举例：山坡上的爬山者\n",
    ">\n",
    "> 假设你站在一座山的某个位置，想要找到最快下山的路线。你会怎么做呢？首先你会注意到山坡的倾斜度（也就是梯度），倾斜越陡的地方，就意味着朝这个方向走可以让你更快地下降海拔。\n",
    ">\n",
    "> 在这个例子中：\n",
    ">\n",
    "> + **你的当前位置**代表模型当前的参数值。\n",
    "> + **山坡的倾斜度**就是梯度，表示你在当前位置向下走的快慢和方向。\n",
    "> + **往斜坡最陡的方向走**相当于使用梯度更新模型参数，使得海拔（也就是损失值）尽快下降。\n",
    ">\n",
    "> 而大模型在应用梯度的概念时，首先会设计一个损失函数，用来衡量模型的预测结果与目标结果的差距。在训练过程中，它通过梯度去帮助每个模型参数不断调整来快速减少损失函数的值，从而提高模型的预测精度。\n",
    "\n",
    "本节中，我们将实现层归一化，以提高神经网络训练的稳定性和效率。\n",
    "\n",
    "归一化的核心思想是将神经网络层的激活（输出）调整为均值为 0，方差为 1（即单位方差）。这种调整可以加速权重的收敛速度，确保训练过程的一致性和稳定性。正如上一节提到的，在 GPT-2 和现代 Transformer 架构中，层归一化通常应用于多头注意力模块的前后以及最终输出层之前。\n",
    "\n",
    "在我们用代码实现层归一化之前，先通过图 4.5 了解一下层归一化的工作原理。\n",
    "\n",
    "<img src=\"../image/chapter4/figure4.5.png\" width=\"75%\" />\n",
    "\n",
    "我们可以通过以下代码重现图 4.5 中的示例，其中实现了一个具有 5 个输入和 6 个输出的神经网络层，并将其应用于两个输入样本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0360974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "# 创建2个训练样本，每个样本有5个维度（特征）\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377ba824",
   "metadata": {},
   "source": [
    "打印出的张量中，第一行表示第一个输入样本的层输出，第二行表示第二个输入样本的层输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96119ae8",
   "metadata": {},
   "source": [
    "我们实现的神经网络层包含一个线性层，后接一个非线性激活函数 ReLU，这是神经网络中的标准激活函数。如果你不熟悉 ReLU，只需了解它的作用是将负值设为 0，确保输出层中没有负值。在 GPT 中，我们将使用另一种更复杂的激活函数，后续章节会介绍。\n",
    "\n",
    "在对这些输出应用层归一化之前，我们先查看其均值和方差："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87fb22dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeeeb5a",
   "metadata": {},
   "source": [
    "以上均值张量的第一行包含第一个输入样本的均值，第二行输出包含第二个输入样本的均值。\n",
    "\n",
    "在计算均值或方差等操作时使用 `keepdim=True` 参数，可以确保输出张量的维度与输入张量相同，即使该操作通过`dim`参数减少了张量的维度。例如，如果不使用 `keepdim=True`，返回的均值张量将是一个二维向量 `[0.1324, 0.2170]`，而使用 `keepdim=True` 后，返回的张量则会是一个 `2×1` 的矩阵 `[[0.1324], [0.2170]]`。\n",
    "\n",
    "`dim` 参数用于指定张量中进行统计计算（如均值或方差）的维度，具体如图 4.6 所示。\n",
    "\n",
    "<img src=\"../image/chapter4/figure4.6.png\" width=\"75%\" />\n",
    "\n",
    "如图 4.6 所示，对于二维张量（如矩阵），在进行均值或方差计算等操作时，使用 `dim=-1` 等同于使用 `dim=1`，因为 `-1` 指的是张量的最后一个维度，即二维张量中的列。在后续对 GPT 模型加入层归一化时，模型会生成形状为 `[batch_size, num_tokens, embedding_size]` 的三维张量，我们依然可以使用 `dim=-1` 对最后一个维度进行归一化，而无需将 `dim=1` 改为 `dim=2`。\n",
    "\n",
    "接下来，我们将对之前获得的层输出应用层归一化。该操作包括减去均值，并除以方差的平方根（即标准差）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a95fba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[-5.9605e-08],\n",
      "        [ 1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cddde34",
   "metadata": {},
   "source": [
    "可以看到，归一化后的层输出现在也包含了负值，其均值为零，方差为 1。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cdb81d",
   "metadata": {},
   "source": [
    "请注意，输出张量中的值`2.9802e-08`是`2.9802 × 10^-8`的科学记数法表示，用十进制形式表示为`0.0000000298`。这个值虽然非常接近 0，但由于计算机表示数字的精度有限，会产生微小的数值误差，因此不完全等于 0。\n",
    "\n",
    "为提高可读性，我们可以将 sci_mode 设置为 False，从而关闭张量值的科学计数法显示模式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a742add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4470e12c",
   "metadata": {},
   "source": [
    "在本节内容中，我们已逐步实现并应用了层归一化。现在将这个过程封装到一个 PyTorch 模块中，以便后续在 GPT 模型中使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a46392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 4.2 A layer normalization class\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ab31b7",
   "metadata": {},
   "source": [
    "以上是对层归一化的具体实现，它作用于输入张量 `x` 的最后一个维度，该维度表示嵌入维度（emb_dim）。变量 `eps` 是一个小常数（epsilon），在归一化过程中加到方差上，以防止出现除零错误。`scale` 和 `shift` 是两个可训练参数（与输入具有相同的维度）。大语言模型（LLM）在训练中会自动调整这些参数，以改善模型在训练任务上的性能。这使得模型能够学习适合数据处理的最佳缩放和偏移方式。\n",
    "\n",
    "> [!NOTE]\n",
    ">\n",
    "> **有偏方差**\n",
    ">\n",
    "> 我们在方差计算方法中选择了设置 `unbiased=False`。对于好奇其含义的读者，可以理解为我们在方差公式中直接用样本数 n 作为分母，不使用贝塞尔校正（通常分母使用 n−1 以校正样本方差估计中的偏差）。这种决定会导致所谓的有偏方差估计。对于大语言模型（LLM）来说，其嵌入维度 n 通常非常大，因此使用 n 和 n−1 的差异实际上可以忽略不计。我们选择这种方式是为了确保与 GPT-2 模型的归一化层兼容，并保持与 TensorFlow 的默认行为一致，后者用于实现最初的 GPT-2 模型。这种设置确保我们的方法与第 6 章中将加载的预训练权重兼容。\n",
    "\n",
    "现在让我们在实践中尝试LayerNorm模块并将其应用于批量输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10719f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059fe4a0",
   "metadata": {},
   "source": [
    "结果表明，层归一化代码运行正常，将两个输入的均值归一化为 0，方差归一化为 1。\n",
    "\n",
    "在本节中，我们介绍了实现 GPT 架构所需的一个基础模块（`LayerNorm`），如图 4.7 所示。\n",
    "\n",
    "<img src=\"../image/chapter4/figure4.7.png\" width=\"75%\" />\n",
    "\n",
    "在下一节中，我们将探讨大语言模型中使用的 GELU 激活函数，它将替代我们在本节使用的传统 ReLU 函数。\n",
    "\n",
    "> [!NOTE]\n",
    ">\n",
    "> **层归一化与批量归一化的区别**\n",
    ">\n",
    "> 如果你熟悉批量归一化这种常见的传统神经网络归一化方法，可能会好奇它与层归一化的区别。与在数量维度上进行归一化的批量归一化不同，层归一化是在特征维度上进行归一化。LLM 通常需要大量计算资源，而可用的硬件资源或特定的使用场景可能会限制训练或推理过程中的批量大小。由于层归一化对每个输入的处理不依赖批量大小，因此在这些场景下提供了更高的灵活性和稳定性。这对于分布式训练或资源受限的环境中部署模型尤其有利。\n",
    "\n",
    "---\n",
    "\n",
    "## 4.3 实现带有 GELU 激活函数的前馈神经网络\n",
    "\n",
    "在本节中，我们将实现一个小型神经网络子模块，作为 LLM 架构中的 Transformer 模块的一部分。我们首先实现 GELU 激活函数，它将在这个神经网络子模块中起着至关重要的作用。（关于在 PyTorch 中实现神经网络的更多信息，请参考附录 A 的 A.5 节：实现多层神经网络）\n",
    "\n",
    "过去，ReLU 激活函数因其简单且有效，常用于各种神经网络架构中。但在大语言模型中，除了传统的 ReLU，还使用了其他几种激活函数，其中两个典型的例子是 GELU（高斯误差线性单元）和 SwiGLU（Swish 门控线性单元）。\n",
    "\n",
    "GELU 和 SwiGLU 是更复杂、平滑的激活函数，分别结合了高斯分布和 sigmoid 门控线性单元。与较简单的 ReLU 不同，这些激活函数能为深度学习模型提供更好的性能。\n",
    "\n",
    "GELU 激活函数可以通过多种方式实现，其确切版本定义为 $GELU(x) = x ⋅ Φ(x)$，其中 $Φ(x)$ 是标准正态分布的累积分布函数。然而在实践中，通常会采用计算开销更低的近似实现（最初的 GPT-2 模型也是用这种近似实现进行训练的）：\n",
    "\n",
    "$$ \\text{GELU}(x) \\approx 0.5 \\cdot x \\cdot\\left(1+\\tanh \\left[\\sqrt{(2 / \\pi)} \\cdot\\left(x+0.044715 \\cdot x^{3}\\right]\\right)\\right. $$\n",
    "\n",
    "我们可以编码将该函数实现为一个 PyTorch 模块，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "376866f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 4.3 An implementation of the GELU activation function\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731d839a",
   "metadata": {},
   "source": [
    "接下来，为了更直观的观察 GELU 函数的形状，并与 ReLU 函数进行对比，我们将这两个函数并排绘制："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff350b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "# 在 -3 到 3 的范围内生成 100 个样本数据点\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7d5fc9",
   "metadata": {},
   "source": [
    "如图 4.8 所示，ReLU 是一个分段线性函数，输入为正时输出输入值本身，否则输出零。而 GELU 是一种平滑的非线性函数，它近似于 ReLU，但在负值上也具有非零梯度。\n",
    "\n",
    "<img src=\"../image/chapter4/figure4.8.png\" width=\"75%\" />\n",
    "\n",
    "如图 4.8 所示，GELU 的平滑性使其在训练过程中具有更好的优化特性，能够对模型参数进行更细微的调整。相比之下，ReLU 在零点处有一个拐角，这在网络深度较大或结构复杂时可能会增加优化难度。此外，ReLU 对所有负输入的输出为零，而 GELU 对负值允许一个小的非零输出。这意味着在训练过程中，接收负输入的神经元也能对学习过程产生一定的贡献，尽管贡献程度不及正输入。\n",
    "\n",
    "接下来让我们使用 GELU 激活函数实现一个小型的神经网络模块 FeedForward，该模块稍后会应用在 LLM 的 Transformer 模块中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e803b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 4.4 A feed forward neural network module\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a65c7",
   "metadata": {},
   "source": [
    "如代码所示，FeedForward 模块是一个小型神经网络，由两个线性层和一个 GELU 激活函数组成。在 1.24 亿参数的 GPT 模型中，该模块可以接收批量输入，每个输入 token 是一个 768 维的向量表示。这一嵌入维度大小通过 `GPT_CONFIG_124M` 配置字典中的 `GPT_CONFIG_124M[\"emb_dim\"]` 参数指定。\n",
    "\n",
    "图 4.9 展示了当我们输入数据后，这个前馈网络内部如何调整嵌入维度。\n",
    "\n",
    "<img src=\"../image/chapter4/figure4.9.png\" width=\"75%\" />\n",
    "\n",
    "按照图 4.9 中的示例，我们初始化一个新的 FeedForward 模块，设置 token 嵌入维度为 768，并输入一个包含 2 个样本且每个样本有 3 个 token 的数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "166133b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "# 创建一个 batch 大小为 2 的示例输入\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a584e55",
   "metadata": {},
   "source": [
    "显然，输出张量的形状与输入张量相同。\n",
    "\n",
    "我们在本节实现的 FeedForward 模块对模型能力的增强（主要体现在从数据中学习模式并泛化方面）起到了关键作用。尽管该模块的输入和输出维度相同，但在内部，它首先通过第一个线性层将嵌入维度扩展到一个更高维度的空间（如图 4.10 所示）。之后再接入非线性 GELU 激活，最后再通过第二个线性层变换回原始维度。这样的设计能够探索更丰富的表示空间。\n",
    "\n",
    "> [!TIP]\n",
    ">\n",
    "> **个人思考：** 这段描述一笔带过了扩展和收缩嵌入维度为模型训练带来的好处，那到底该如何理解这样的设计能够探索更丰富的表示空间呢？\n",
    ">\n",
    "> 可以将扩展和收缩的过程类比为一种**数据解压缩与重新压缩**的机制：\n",
    ">\n",
    "> + **扩展（解压缩）**：假设我们有一段压缩的音乐文件（例如 MP3），里面包含了音频的基本信息。通过解压缩（扩展），我们把这个文件变成了一个更高质量的音频格式，允许我们看到（听到）更多的细节，比如乐器的细微声响和音调变化。\n",
    "> + **特征提取**：接着，我们可以在这个高质量的音频文件中应用各种音频处理算法（相当于非线性激活），分析出更多细节，比如每种乐器的声音特点。\n",
    "> + **收缩（压缩）**：最后，我们将音频再次压缩为一种更适合传输和存储的格式。虽然最终文件变小了，但这个文件已经包含了之前提取出的更多的声音细节。\n",
    ">\n",
    "> 将这种理解再应用到神经网络中，扩展后的高维空间可以让模型“看到”输入数据中更多的隐藏特征，提取出更丰富的信息。然后在收缩回低维度时，这些丰富的特征被整合到了输入的原始维度表示中，使模型最终的输出包含更多的上下文和信息。\n",
    "\n",
    "<img src=\"../image/chapter4/figure4.10.png\" width=\"75%\" />\n",
    "\n",
    "此外，输入输出维度保持一致也有助于简化架构，方便堆叠多层（在后续的章节实现），无需调整各层维度，从而提升了模型的可扩展性。\n",
    "\n",
    "如图4.11所示，我们目前已经实现了LLM 架构中的大部分模块。\n",
    "\n",
    "<img src=\"../image/chapter4/figure4.11.png\" width=\"75%\" />\n",
    "\n",
    "下一节，我们将介绍“快捷连接”的概念，即在神经网络的不同层之间插入的连接结构，它对于提升深度神经网络架构的训练性能非常重要。\n",
    "\n",
    "---\n",
    "\n",
    "## 4.4 添加快捷连接\n",
    "\n",
    "接下来，我们来讨论快捷连接（也称跳跃连接或残差连接）的概念。快捷连接最初是在计算机视觉中的深度网络（尤其是残差网络）提出的，用于缓解梯度消失问题。梯度消失是指在训练中指导权重更新的梯度在反向传播过程中逐渐减小，导致早期层（靠近输入端的网络层）难以有效训练，如图 4.12 所示。\n",
    "\n",
    "<img src=\"../image/chapter4/figure4.12.png\" width=\"75%\" />\n",
    "\n",
    "如图 4.12 所示，快捷连接通过跳过一层或多层，为梯度提供一条更短的流动路径，这是通过将某层的输出加到后续层的输出上来实现的。因此，这种连接方式也称为跳跃连接。在反向传播中，快捷连接对保持梯度流动至关重要。\n",
    "\n",
    "在以下代码示例中，我们将实现图 4.12 中所示的神经网络，以展示如何在前向传播方法中添加快捷连接："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0eabf90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 4.5 A neural network to illustrate shortcut connections\n",
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            # Implement 5 layers\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of the current layer\n",
    "            layer_output = layer(x)\n",
    "            # Check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e546e1e9",
   "metadata": {},
   "source": [
    "以上代码实现了一个 5 层的深度神经网络，每层包括一个线性层和 GELU 激活函数。在前向传播中，我们将输入逐层传递，同时如果 `self.use_shortcut` 属性设置为 True，则会添加图 4.12 所示的快捷连接。\n",
    "\n",
    "我们将使用以下代码初始化一个没有快捷连接的神经网络，其中每一层都被初始化为接受 3 个输入值并返回 3 个输出值。最后一层则返回一个单一的输出值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5dd8bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123) # specify random seed for the initial weights for reproducibility\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d209d4a",
   "metadata": {},
   "source": [
    "接下来，我们实现一个用于在模型反向传播过程中计算梯度的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf724fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss based on how close the target\n",
    "    # and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "\n",
    "    # Backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # Print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f1e3e",
   "metadata": {},
   "source": [
    "上述代码中，我们定义了一个损失函数，用来计算模型输出与用户指定目标（此处为简单起见，目标值设为 0）之间的差距。接着，当调用 `loss.backward()` 时，PyTorch 会为模型的每一层计算损失的梯度。我们可以通过 `model.named_parameters()` 遍历权重参数。假设某层的权重参数是一个 3×3 的矩阵，那么这一层会有 3×3 的梯度值。然后我们打印出这 3×3 梯度值的绝对均值，以便得到每层的单一梯度值，从而更容易比较各层之间的梯度大小。\n",
    "\n",
    "简而言之，`.backward()` 是 PyTorch 中一个用于自动计算损失梯度的便捷方法，这在模型训练过程中很重要。它让我们无需亲自实现梯度计算的数学过程，从而大大简化了深度神经网络的开发过程。如果您对梯度和神经网络训练不熟悉，建议参考附录 A 中的 A.4 节：**轻松实现自动微分** 和 A.7 节：**典型的训练循环**。\n",
    "\n",
    "现在让我们使用 `print_gradients` 函数，并将其应用到没有跳跃连接的模型上："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f6b5d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041071094573\n",
      "layers.3.0.weight has gradient mean of 0.0013988735154271126\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e1c7f6",
   "metadata": {},
   "source": [
    "从 `print_gradients` 函数的输出可以看出，梯度在从最后一层（layers.4）到第一层（layers.0）时逐渐减小，这种现象称为梯度消失问题。\n",
    "\n",
    "我们再来创建一个带有跳跃连接的模型，看看它的表现如何："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4ed60f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f88b2d",
   "metadata": {},
   "source": [
    "从输出结果可以看到，最后一层（layers.4）的梯度依然比其他层更大。然而，随着接近第一层（layers.0），梯度值逐渐趋于稳定，并未缩小到几乎消失的程度。\n",
    "\n",
    "总之，快捷连接在解决深度神经网络中的梯度消失问题方面具有重要作用。作为 LLM 的核心构建单元，快捷连接可以确保各层之间的梯度稳定流动，从而帮助 GPT 模型更有效的训练（下一章实现训练过程）。\n",
    "\n",
    "在介绍了快捷连接后，我们将在下一节把之前讲解的所有概念（层归一化、GELU 激活、前馈网络模块和快捷连接）整合进一个 Transformer 模块中，这是构建 GPT 架构所需的最后一个模块。\n",
    "\n",
    "> [!TIP]\n",
    ">\n",
    "> **个人思考：** 看到这里，不知各位读者是否真正理解了快捷连接在深度神经网络中的作用，这里其实涉及到快捷连接的两个重要的作用：\n",
    ">\n",
    "> + **保持信息（或者说是特征）流畅传递**\n",
    "> + **缓解梯度消失问题**\n",
    ">\n",
    "> 让我们逐一解读，LLM 中的每个Transformer 模块通常包含两个重要组件（**可以先阅读完4.5节，再回头看这里的解读**）：\n",
    ">\n",
    "> 1. **自注意力层（Self-Attention Layer）**：计算每个 token 与其他 token 的关联，帮助模型理解上下文。\n",
    "> 2. **前馈网络（Feed Forward Network）**：对每个 token 的嵌入（embedding）进行进一步的非线性转换，使模型能够提取更复杂的特征。\n",
    ">\n",
    "> 这两个部分都在层归一化（Layer Normalization）和快捷连接（Shortcut Connections）的配合下工作。\n",
    ">\n",
    "> 假设我们正在训练一个 LLM ，并希望它理解下面的句子：\n",
    ">\n",
    "> `The cat sat on the mat because it was tired.`\n",
    ">\n",
    "> 模型需要通过多个 Transformer 层来逐层处理该句子，使得每个词（token）在上下文中能被理解。为了达到这一目的，每个 token 的嵌入会在多层中进行注意力计算和前馈网络处理。\n",
    ">\n",
    "> 1. **没有快捷连接时的情况**\n",
    ">\n",
    ">    如果没有快捷连接，那么每个 Transformer 层的输出就直接传递到下一个层。这种情况下，网络中的信息流大致如下：\n",
    ">\n",
    ">    + **层间信息传递的局限**：假设当前层的注意力机制计算出了“it”和“cat”之间的关系，如果前馈网络进一步转换了这个信息，那么下一层就只能基于该层的输出，可能丢失一些最初的语义信息。\n",
    ">    + **梯度消失**：在训练过程中，梯度从输出层逐层向回传播。如果层数过多，梯度会逐渐变小（即“梯度消失”），从而导致模型难以有效更新前面层的参数。\n",
    ">\n",
    ">    这种情况下，由于信息不能直接流动到更深层次的网络，可能会导致模型难以有效捕捉到前层的一些原始信息。\n",
    ">\n",
    "> 2. **加入快捷连接后的情况**\n",
    ">\n",
    ">    加入快捷连接后，信息可以在层与层之间**直接跳跃**。例如，假设在第 $n$ 层，我们有输入 $X_n$，经过注意力和前馈网络得到输出 $F(X_n)$。加入快捷连接后，这一层的输出可以表示为：\n",
    ">\n",
    ">    $$\\text { 输出 }=X_{n}+F\\left(X_{n}\\right)$$\n",
    ">\n",
    ">    这意味着第 n 层的输出不仅包含了这一层的新信息 $F(X_n)$，还保留了原始输入 $X_n$ 的信息。下面是这样做的好处：\n",
    ">\n",
    ">    - **保留原始信息**\n",
    ">\n",
    ">      快捷连接让输入的原始信息直接传递到后续层，避免了在多层处理过程中丢失重要信息。例如，“it” 和 “cat” 之间的关系在较浅层中被捕捉到后，即使后面的层有进一步的处理，模型依然能够从快捷连接中获得最初的上下文信息。\n",
    ">\n",
    ">    - **减轻梯度消失**\n",
    ">\n",
    ">      假设我们有一个简单的三层网络，第三层的输出 O 是整个网络的输出。我们从损失函数 LLL 开始计算梯度：\n",
    ">\n",
    ">      - 根据反向传播的原理，**无快捷连接**时，梯度必须逐层传递，如下：\n",
    ">\n",
    ">        $$\\frac{\\partial L}{\\partial X_{1}}=\\frac{\\partial L}{\\partial X_{3}} \\cdot \\frac{\\partial X_{3}}{\\partial X_{2}} \\cdot \\frac{\\partial X_{2}}{\\partial X_{1}}$$\n",
    ">\n",
    ">        这里，如果某一层的梯度值很小，那么梯度会被逐层缩小，导致梯度消失。\n",
    ">\n",
    ">      - **有快捷连接**时，假设我们在每一层之间都添加快捷连接，梯度的传播路径就多了一条直接路径：\n",
    ">\n",
    ">        $$\\frac{\\partial L}{\\partial X_{1}}=\\frac{\\partial L}{\\partial\\left(X_{1}+F\\left(X_{1}\\right)\\right)} \\cdot\\left(1+\\frac{\\partial F\\left(X_{1}\\right)}{\\partial X_{1}}\\right)$$\n",
    ">\n",
    ">        这样，即使 $` \\frac{\\partial F\\left(X_{1}\\right)}{\\partial X_{1}} `$ 很小，梯度依然可以通过 111 这条路径直接传递到更前面的层。\n",
    "\n",
    "---\n",
    "\n",
    "## 4.5 在 Transformer 模块中连接注意力层与线性层\n",
    "\n",
    "本节我们将实现 Transformer 模块，它是 GPT 和其他大语言模型架构的基本模块。这个在 124M 参数的 GPT-2 架构中重复了十几次的模块，结合了多头注意力、层归一化、dropout、前馈层和 GELU 激活等多个概念，详见图 4.13。在下一节中，我们将把这个 Transformer 模块连接到 GPT 架构的其余部分。\n",
    "\n",
    "<img src=\"../image/chapter4/figure4.13.png\" width=\"75%\" />\n",
    "\n",
    "如图 4.13 所示，Transformer 模块结合了多个组件，包括第 3 章中的掩码多头注意力模块以及我们在 4.3 节中实现的前馈网络模块。\n",
    "\n",
    "当 Transformer 模块处理输入序列时，序列中的每个元素（如单词或子词 token）都会被表示为固定大小的向量（如图 4.13 中为 768 维）。Transformer 模块中的操作，包括多头注意力和前馈层，旨在以维度不变的方式对这些向量进行转换。\n",
    "\n",
    "之所以这样设计，是因为多头注意力模块中的自注意力机制用于识别和分析输入序列中各元素之间的关系，而前馈神经网络则对输入序列中每个位置的数据单独进行修改。这种组合不仅能够更细致地理解和处理输入信息，还增强了模型处理复杂数据模式的整体能力。\n",
    "\n",
    "可以通过以下代码实现 Transformer 模块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "781ac8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 4.6 The transformer block component of GPT\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,\n",
    "                 context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "             torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # 对每个注意力头进行点积运算\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # 使用掩码填充注意力分数\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # (b, num_heads, num_tokens, num_tokens) @ (b, num_heads, num_tokens, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # 将多个注意力头的输出结果合并，其中输出维度 self.d_out 等于注意力头数 self.num_heads 与每个头的维度 self.head_dim 的乘积\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "\n",
    "        # 使用线性层组合头部输出\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 注意力模块中的快捷连接\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "        # 前馈网络模块中的快捷链接\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        # 将原始输入加回到输出中\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781fe8f3",
   "metadata": {},
   "source": [
    "给定的代码在 PyTorch 中定义了一个 TransformerBlock 类，包含多头注意力机制（MultiHeadAttention）和前馈网络（FeedForward），并根据提供的配置字典（cfg）进行配置，例如前文定义的 GPT_CONFIG_124M。\n",
    "\n",
    "层归一化（LayerNorm）在这两个组件（即自注意力和前馈网络）之前应用，而 dropout 则在它们之后应用，用于正则化模型并防止过拟合。这种方式也称为前置层归一化（Pre-LayerNorm）。而在早期的架构中（如原始的 Transformer 模型），一般将层归一化应用在自注意力和前馈网络之后，这被称为后置层归一化（Post-LayerNorm），这种方式通常会导致较差的训练效果。\n",
    "\n",
    "该类还实现了前向传播（`forward方法`），其中每个组件后面都设有一个快捷连接，将对应组件的输入添加到输出中。这一关键特性有助于在训练过程中促进梯度流动，从而提升 LLM 的学习能力，相关内容详见第 4.4 节。\n",
    "\n",
    "现在使用我们之前定义的 `GPT_CONFIG_124M` 配置字典，实例化一个 Transformer 模块，并向其中输入一些示例数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6986f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "# 建一个形状为 [batch_size, num_tokens, emb_dim] 的输入张量\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8face85",
   "metadata": {},
   "source": [
    "从代码输出可以看出，Transformer 模块的输出维度与输入维度保持一致，这说明 Transformer 架构在处理序列数据时不会改变数据的形状。\n",
    "\n",
    "Transformer 模块结构中保持数据形状不变并非偶然，而是其设计的一个关键特性。这种设计使 Transformer 擅长处理各种序列到序列任务，因为每个输出向量直接对应一个输入向量，保持一一对应关系。然而，虽然维度一致，但输出向量是包含整个输入序列信息的“上下文向量”。也就是说，尽管序列的物理维度（长度和特征维度）在经过 Transformer 模块时保持不变，但每个输出向量的内容会被重新编码，融合整个输入序列的上下文信息。\n",
    "\n",
    "在本节完成了 Transformer 模块的实现后，我们已经具备了实现 GPT 架构所需的全部基础模块（如图 4.14 所示）。\n",
    "\n",
    "<img src=\"../image/chapter4/figure4.14.png\" width=\"75%\" />\n",
    "\n",
    "如图 4.14 所示，Transformer 模块由层归一化、带有 GELU 激活函数的前馈网络和快捷连接组成，这些内容在本章前面已经讨论过。正如我们将在接下来的章节中看到的，这个 Transformer 模块将构成我们要实现的 GPT 架构的核心部分。\n",
    "\n",
    "---\n",
    "\n",
    "## 4.6 实现 GPT 模型\n",
    "\n",
    "截止到目前，本章已初步实现了一个名为`DummyGPTModel`类的GPT架构，在该`DummyGPTModel`的代码实现中，我们展示了 GPT 模型的输入和输出形式，但其内部的一些核心模块仅仅使用了`DummyTransformerBlock`和`DummyLayerNorm`等类来占位，并未替换成真正的实现。\n",
    "\n",
    "在本节中，我们将 DummyTransformerBlock 和 DummyLayerNorm 占位符替换为本章后面实现的真实 TransformerBlock 和 LayerNorm 类，以组装出一个完整可用的原始 GPT-2 模型（124M 参数版本）。在第 5 章，我们将预训练一个 GPT-2 模型，第 6 章则会加载 OpenAI 的预训练权重。\n",
    "\n",
    "在我们通过代码构建 GPT-2 模型之前，先通过图 4.15 看一下模型的整体结构，该结构结合了本章目前为止介绍的所有概念。\n",
    "\n",
    "<img src=\"../image/chapter4/figure4.15.png\" width=\"75%\" />\n",
    "\n",
    "如图 4.15 所示，我们在 4.5 节中编写的 Transformer 模块在 GPT 架构中会重复多次。在参数量为 1.24 亿的 GPT-2 模型中，该模块重复了 12 次，这一数量通过 `GPT_CONFIG_124M` 配置字典中的`n_layers`参数指定。在 GPT-2 最大的 15.42 亿参数模型中，Transformer 模块重复了 36 次。\n",
    "\n",
    "我们还可以从图 4.15 中得知，最后一个 Transformer 模块的输出会经过一个最终的层归一化步骤，然后进入线性输出层。该层将 Transformer 的输出映射到一个高维空间（在本例中为 50,257 维，对应于模型的词汇表大小），以预测序列中的下一个词。\n",
    "\n",
    "接下来我们用代码实现图 4.15 中的架构："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d701ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 4.7 The GPT model architecture implementation\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "\n",
    "        # 设备设置将根据输入数据所在的位置选择在 CPU 或 GPU 上训练模型\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee26828",
   "metadata": {},
   "source": [
    "通过代码可以看出，由于我们已经在 4.5 节实现了 `TransformerBlock` 类，从而使得 `GPTModel` 类的设计更为简洁。\n",
    "\n",
    "`GPTModel` 类的构造函数 `__init__` 使用字典 `cfg` 中的配置参数初始化 token 嵌入层和位置嵌入层。这些嵌入层负责将输入的 token 索引转换为密集向量并加入位置信息（在第 2 章已讨论过）。\n",
    "\n",
    "接下来，`__init__` 方法会根据 `cfg` 中指定的层数创建一个由 TransformerBlock 模块组成的顺序堆栈。紧接在 TransformerBlock 堆栈之后应用一个 LayerNorm 层，对其输出进行标准化，从而稳定训练过程。最后，定义了一个无偏置的线性输出层，将 Transformer 的输出投射到分词器的词汇空间，为词汇表中的每个 token 生成对应的 logits。\n",
    "\n",
    "forward 方法则负责接收一批 token 索引作为输入，计算它们的词嵌入向量，并应用位置嵌入，接着将序列通过 Transformer 模块进行处理，对最终的输出进行归一化，最后计算 logits 来表示下一个 token 的非归一化概率。我们将在下一节将这些 logits 转换为 token 和文本输出。\n",
    "\n",
    "现在让我们使用 `GPT_CONFIG_124M` 字典配置来初始化一个具有 1.24 亿参数的 GPT 模型，并将本章开头创建的批量文本作为模型输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "068f4038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4223, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c2c89e",
   "metadata": {},
   "source": [
    "可以看到，输出张量的形状是 [2, 4, 50257]，这是因为我们输入了 2 个文本，每个文本包含 4 个 token。最后一个维度 50257 对应于分词器的词汇表大小。在下一节中，我们将看到如何将这些 50257 维的输出向量转换回 token。\n",
    "\n",
    "在我们继续后续内容并编写将模型输出转换为文本的函数之前，让我们先花点时间研究一下模型架构本身，并分析其规模。\n",
    "\n",
    "使用 numel() 方法（即 `number of elements` 的缩写），可以统计模型中参数张量的总参数量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b3069d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef320bc5",
   "metadata": {},
   "source": [
    "细心的读者可能会发现一个差异：我们之前提到 GPT 模型的参数量为 1.24 亿，但代码输出的实际参数量却是 1.63 亿，这是为什么呢？\n",
    "\n",
    "原因在于 GPT-2 架构中使用了一种称为‘权重共享’的概念，这意味着 GPT-2 架构将 token 嵌入层的权重复用于输出层。为了更好地理解这一点，我们可以来看一下在模型中初始化的 token 嵌入层和线性输出层的形状："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ad9c618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9a0fa8",
   "metadata": {},
   "source": [
    "token 嵌入层和输出层的参数量很大，因为分词器词汇表中包含 50,257 个 token。根据权重共享原则，我们可以从 GPT-2 模型的总参数量中去除输出层的参数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e3a32d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f19746",
   "metadata": {},
   "source": [
    "如我们所见，模型现在的参数量为 1.24 亿，与 GPT-2 原始模型的规模一致。\n",
    "\n",
    "权重共享能够减少模型的整体内存占用和计算复杂度。然而，根据我的经验，分别使用独立的 token 嵌入层和输出层会使训练效果和模型性能更佳，因此在我们的 GPT 模型实现中，我们使用了独立的嵌入层和输出层。现代大语言模型也是如此。不过，在第 6 章加载 OpenAI 的预训练权重时，我们会再次探讨并实现权重共享的概念。\n",
    "\n",
    "> [!NOTE]\n",
    ">\n",
    "> **练习 4.1 前馈网络和注意力模块的参数数量**\n",
    ">\n",
    "> 计算并比较前馈模块和多头注意力模块中包含的参数数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b666d00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前馈网络参数量: 56,669,184\n",
      "多头注意力参数量: 28,320,768\n",
      "参数量比例 (前馈网络/多头注意力): 2.00\n"
     ]
    }
   ],
   "source": [
    "# 计算前馈网络模块的参数量\n",
    "ff_params = 0\n",
    "att_params = 0\n",
    "for block in model.trf_blocks:\n",
    "    ff_params += sum(p.numel() for p in block.ff.parameters())\n",
    "    att_params += sum(p.numel() for p in block.att.parameters())\n",
    "print(f\"前馈网络参数量: {ff_params:,}\")\n",
    "print(f\"多头注意力参数量: {att_params:,}\")\n",
    "print(f\"参数量比例 (前馈网络/多头注意力): {ff_params/att_params:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4300f350",
   "metadata": {},
   "source": [
    "最后，让我们来计算 GPTModel 对象中 1.63 亿参数所需的内存："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bbd7018e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "# 计算参数总大小（假设每个参数为 float32 类型，占用 4 字节）\n",
    "total_size_bytes = total_params * 4\n",
    "# 转换为 MB\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285c15a9",
   "metadata": {},
   "source": [
    "通过计算 GPTModel 中 1.63 亿个参数所需的内存，并假设每个参数为 32 位浮点数，占用 4 字节，我们得出模型总大小为 621.83 MB。这表明，即使是相对较小的大语言模型也需要较大的存储空间。\n",
    "\n",
    "在本节中，我们实现了 GPTModel 架构，并观察到它的输出是形状为 [batch_size, num_tokens, vocab_size] 的数值张量。接下来，我们将编写代码把这些输出张量转换为文本。\n",
    "\n",
    "> [!NOTE]\n",
    ">\n",
    "> **练习 4.2 初始化大型 GPT 模型**\n",
    ">\n",
    "> 本章中，我们初始化了一个拥有 1.24 亿参数的 GPT 模型，即 GPT-2 small。请在不更改代码的情况下（仅更新配置文件），使用 `GPTModel` 类实现 GPT-2 medium（1024 维嵌入、24 层 Transformer 块、16 个多头注意力头）、GPT-2 large（1280 维嵌入、36 层 Transformer 块、20 个多头注意力头）和 GPT-2 XL（1600 维嵌入、48 层 Transformer 块、25 个多头注意力头）。作为附加任务，请计算每个 GPT 模型的总参数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0dc08790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 406,212,608\n"
     ]
    }
   ],
   "source": [
    "GPT_CONFIG_MEDIUM = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 1024,         # Embedding dimension\n",
    "    \"n_heads\": 16,          # Number of attention heads\n",
    "    \"n_layers\": 24,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_medium = GPTModel(GPT_CONFIG_MEDIUM)\n",
    "total_params_medium = sum(p.numel() for p in model_medium.parameters())\n",
    "print(f\"Total number of parameters: {total_params_medium:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b55f4d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 838,220,800\n"
     ]
    }
   ],
   "source": [
    "GPT_CONFIG_LARGE = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 1280,         # Embedding dimension\n",
    "    \"n_heads\": 20,          # Number of attention heads\n",
    "    \"n_layers\": 36,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_large = GPTModel(GPT_CONFIG_LARGE)\n",
    "total_params_large = sum(p.numel() for p in model_large.parameters())\n",
    "print(f\"Total number of parameters: {total_params_large:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ef879fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1,637,792,000\n"
     ]
    }
   ],
   "source": [
    "GPT_CONFIG_XL = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 1600,         # Embedding dimension\n",
    "    \"n_heads\": 25,          # Number of attention heads\n",
    "    \"n_layers\": 48,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_xl = GPTModel(GPT_CONFIG_XL)\n",
    "total_params_xl = sum(p.numel() for p in model_xl.parameters())\n",
    "print(f\"Total number of parameters: {total_params_xl:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdcae72",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.7 生成文本\n",
    "\n",
    "在本章的最后一节，我们将编写代码把 GPT 模型的张量输出转回文本。在开始之前，我们先简要回顾一下像 LLM 这样的生成模型是如何逐词生成文本的，如图 4.16 所示。\n",
    "\n",
    "<img src=\"../image/chapter4/figure4.16.png\" width=\"75%\" />\n",
    "\n",
    "如图 4.16 所示，GPT 模型在给定输入上下文（例如 ‘Hello, I am’）后，逐步生成文本。每次迭代中，输入上下文会不断扩展，使模型能够生成连贯且符合上下文的内容。在第 6 次迭代时，模型已构建出完整句子 ‘Hello, I am a model ready to help.’。\n",
    "\n",
    "在上一节，我们看到目前的 GPTModel 输出的张量形状为 `[batch_size, num_token, vocab_size]`。那么问题来了，GPT 模型是如何将这些输出张量转化为图 4.16 所示的生成文本的呢？\n",
    "\n",
    "GPT 模型从输出张量到生成文本的过程涉及几个步骤（如图 4.17 所示）。这些步骤包括解码输出张量、根据概率分布选择 token，并将其转化为可读文本。\n",
    "\n",
    "<img src=\"../image/chapter4/figure4.17.png\" width=\"75%\" />\n",
    "\n",
    "图 4.17 详细展示了 GPT 模型根据输入生成下一个 token 的单步过程。\n",
    "\n",
    "在每一步，模型会输出一个矩阵，其中的向量表示潜在的下一个 token。取出对应于下一个 token 的向量，并通过 softmax 函数将其转换为概率分布。在包含概率分数的向量中，找到最高值的索引，并将其转换为 token ID。将该 token ID 解码回文本，得到序列中的下一个 token。最后，将该 token 添加到先前的输入中，形成下一次迭代的新输入序列。这种逐步生成的过程使模型能够根据初始输入上下文，按顺序生成文本，从而构建出连贯的短语和句子。\n",
    "\n",
    "在实践中，我们会多次迭代这一过程（如前文图 4.16 所示），直到生成的 token 数量达到用户指定值。\n",
    "\n",
    "我们通过以下代码来实现上述的 token 生成过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8ca1f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 4.8 A function for the GPT model to generate text\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size): #A\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]                           #B\n",
    "        with torch.no_grad():\n",
    "           logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]                                   #C\n",
    "        probas = torch.softmax(logits, dim=-1)                      #D\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)       #E\n",
    "        idx = torch.cat((idx, idx_next), dim=1)                     #F\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "#A idx 是当前上下文中索引的数组，形状为 (batch, n_tokens)\n",
    "#B 若上下文长度超出支持范围，则进行裁剪。例如，若模型仅支持 5 个 token，而上下文长度为 10，仅使用最后 5 个 token 作为上下文\n",
    "#C 仅关注最后一个时间步，将形状从 (batch, n_token, vocab_size) 转换为 (batch, vocab_size)\n",
    "#D probas 的形状为 (batch, vocab_size)\n",
    "#E idx_next 的形状为 (batch, 1)\n",
    "#F 将采样的索引追加到当前序列中，此时 idx 的形状为 (batch, n_tokens+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf1cba7",
   "metadata": {},
   "source": [
    "在上述代码中，`generate_text_simple` 函数使用 Softmax 函数将 logits 转换为概率分布，然后通过 `torch.argmax` 找出概率最高的位置。Softmax 函数是单调的，这意味着它会保持输入的相对顺序，因此，Softmax 这一步实际上是冗余的，因为 Softmax 输出中最高值的位置与原始 logits 中最高值的位置相同。换句话说，我们可以直接对 logits 应用 `torch.argmax` 得到相同的结果。不过，我们保留了这个转换过程，以展示从 logits 到概率的完整过程，有助于理解模型如何生成最可能的下一个词，这种方式称为贪婪解码。\n",
    "\n",
    "在下一章中，我们将在实现 GPT 训练代码的同时，引入一些新的采样技术，通过修改 softmax 输出，使模型在生成文本时不总是选择概率最高的词，这可以增加文本的多样性和创造性。\n",
    "\n",
    "我们可以使用 `generate_text_simple` 函数逐步生成 token ID，每次生成一个 token ID 并将其附加到上下文中。其具体过程详见图 4.18（每次迭代生成 token ID 的步骤详见图 4.17）。\n",
    "\n",
    "<img src=\"../image/chapter4/figure4.18.png\" width=\"75%\" />\n",
    "\n",
    "如图 4.18 所示，我们以迭代的方式逐步生成 token ID。例如，在第 1 轮迭代中，模型接收到“Hello , I am”对应的 token 作为输入，预测下一个 token（ID 为 257，对应“a”），并将其添加到输入序列中。这个过程不断重复，直到模型在第六轮迭代后生成完整的句子“Hello, I am a model ready to help.”。\n",
    "\n",
    "接下来我们将实践 `generate_text_simple` 函数，并使用 ‘Hello, I am’ 作为模型的输入上下文，具体如图 4.18 所示。\n",
    "\n",
    "首先，我们将输入上下文编码为 token ID："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a05b9fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "# 添加批次维度\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133675ad",
   "metadata": {},
   "source": [
    "接下来，将模型置于 `.eval()` 模式，禁用训练时使用的随机组件（如 dropout），然后在编码后的输入张量上使用 `generate_text_simple` 函数进行文本生成："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "948a1df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 25079, 40114,  5797, 40538, 43588, 42500]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "# 禁用 dropout，因为当前不是在训练模型\n",
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9ff7aa",
   "metadata": {},
   "source": [
    "接着，使用分词器的 `.decode` 方法可以将 ID 转回文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "144d5d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am turnover watt agent 1906north Finder\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aa9ff5",
   "metadata": {},
   "source": [
    "从以上的输出可以看到，模型生成的是一些毫无意义的内容，完全不像图 4.18 中的连贯文本。这是为什么呢？原因在于模型还没有经过训练。到目前为止，我们只是实现了 GPT 架构，并用随机权重初始化了模型实例。\n",
    "\n",
    "模型训练本身就是一个庞大的主题，我们将在下一章详细讨论。\n",
    "\n",
    "> [!NOTE]\n",
    ">\n",
    "> **练习 4.3：使用独立的 Dropout 参数**\n",
    ">\n",
    "> 在本章开头，我们在 `GPT_CONFIG_124M` 字典中定义了一个全局的 `\"drop_rate\"` 设置，用于统一设置整个 GPTModel 架构中各处的 dropout 率。请修改代码，为模型架构中各个不同的 dropout 层指定独立的 dropout 值。（提示：我们在三个不同的地方使用了dropout层：嵌入层、快捷连接层和多头注意力模块）\n",
    "\n",
    "---\n",
    "\n",
    "## 4.8 本章摘要\n",
    "\n",
    "+ 层归一化通过确保每一层的输出具有一致的均值和方差，从而稳定训练过程。\n",
    "+ 在大语言模型（LLM）中，快捷连接可以通过将某一层的输出直接传递给更深层来跳过一个或多个层，有助于缓解深度神经网络训练中的梯度消失问题。\n",
    "+ Transformer 模块是 GPT 模型的核心结构，结合了掩码多头注意力模块和使用 GELU 激活函数的全连接前馈网络。\n",
    "+ GPT 模型是由许多重复的 Transformer 模块组成的大语言模型，参数量高达数百万到数十亿。\n",
    "+ GPT 模型有不同的规模，例如 1.24 亿、3.45 亿、7.62 亿和 15.42 亿参数。这些不同规模的模型可以用同一个 GPTModel 类来实现。\n",
    "+ 类似 GPT 的大语言模型通过逐个预测 token，根据给定的输入上下文，将输出张量解码为可读文本，从而实现文本生成能力。\n",
    "+ 未经训练的 GPT 模型生成的文本往往语义不连贯，这突显了训练对于生成连贯文本的重要性，这也将是后续章节的讨论重点。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
