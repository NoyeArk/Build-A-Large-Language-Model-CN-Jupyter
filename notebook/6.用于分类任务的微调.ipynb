{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cc513a3",
   "metadata": {},
   "source": [
    "# 6. 用于分类任务的微调\n",
    "\n",
    "本章涵盖以下内容：\n",
    "\n",
    "+ **介绍不同的LLM微调方法**\n",
    "+ **准备用于文本分类任务的数据集**\n",
    "+ **调整预训练的 LLM 以便微调**\n",
    "+ **微调 LLM 以识别垃圾短信**\n",
    "+ **评估微调后的 LLM 分类器的准确性**\n",
    "+ **使用微调后的 LLM 对新数据进行分类**\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "- [6.1 不同类型的微调](#61-不同类型的微调)\n",
    "- [6.2 准备数据集](#62-准备数据集)\n",
    "- [6.3 创建数据加载器](#63-创建数据加载器)\n",
    "- [6.4 使用预训练权重初始化模型](#64-使用预训练权重初始化模型)\n",
    "- [6.5 添加分类头](#65-添加分类头)\n",
    "- [6.6 计算分类损失和准确率](#66-计算分类损失和准确率)\n",
    "- [6.7 使用监督数据对模型进行微调](#67-使用监督数据对模型进行微调)\n",
    "- [6.8 将 LLM 用于垃圾短信分类](#68-将-llm-用于垃圾短信分类)\n",
    "- [6.9 本章摘要](#69-本章摘要)\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "\n",
    "在之前的章节中，我们实现了 LLM 的架构，进行了预训练，并学习了如何从外部来源（如 OpenAI）导入预训练权重。本章将在此基础上，通过微调 LLM 来完成特定目标任务，比如文本分类（见图 6.1）。我们将以一个具体的例子来说明如何将文本消息分类为垃圾短信或正常短信。\n",
    "\n",
    "<img src=\"../Image/chapter6/figure6.1.png\" width=\"75%\" />\n",
    "\n",
    "图 6.1 展示了微调 LLM 的两种主要方式：用于分类的微调（步骤 8）和用于指令遵循的微调（步骤 9）。在下一节中，我们将深入探讨这两种微调方式。\n",
    "\n",
    "\n",
    "\n",
    "## 6.1 不同类型的微调\n",
    "\n",
    "微调语言模型最常见的方法是指令微调和分类微调。指令微调通过在一组任务上使用特定指令训练模型，用以提升模型对自然语言提示中任务描述的理解和执行能力，如图 6.2 所示。\n",
    "\n",
    "<img src=\"../Image/chapter6/figure6.2.png\" width=\"75%\" />\n",
    "\n",
    "下一章将讨论指令微调，相关内容在图 6.2 中有所展示。而本章的重点是分类微调，如果您有机器学习基础，可能已经对这一概念比较熟悉。\n",
    "\n",
    "在分类微调中，模型被训练用来识别特定的一组类别标签，比如“垃圾短信”和“非垃圾短信”。分类任务的应用不仅限于 LLM 和电子邮件过滤，还包括从图像中识别不同种类的植物、将新闻分类到体育、政治或科技等主题，以及在医学影像中区分良性和恶性肿瘤。\n",
    "\n",
    "但有一个关键点需要注意，经过分类微调的模型只能预测训练中遇到的类别。例如，它可以判断某内容是‘垃圾短信’还是‘非垃圾短信’（如图 6.3 所示），但不能对输入文本提供其他方面的信息。\n",
    "\n",
    "<img src=\"../Image/chapter6/figure6.3.png\" width=\"75%\" />\n",
    "\n",
    "与图6.3中所示的分类微调模型不同，指令微调模型通常可以执行更广泛的任务。分类微调模型可以视为高度专业化的模型，而相比之下，开发一个适用于各种任务的通用型模型通常更具挑战性。\n",
    "\n",
    "> [!NOTE]\n",
    ">\n",
    "> **选择合适的微调方式**\n",
    ">\n",
    "> 指令微调提升了模型基于用户指令进行理解和生成响应的能力。它适用于需要基于复杂用户指令处理多任务的模型，增强模型的灵活性和交互质量。而分类微调则适合需要将数据精确分类为预定义类别的任务，例如情感分析或垃圾短信检测。\n",
    ">\n",
    "> 虽然指令微调用途更广泛，但需要更大的数据集和更多的计算资源，才能训练出能胜任多种任务的模型。相比之下，分类微调所需的数据和计算量更少，但用途局限于模型已训练的特定类别。\n",
    "\n",
    "---\n",
    "\n",
    "## 6.2 准备数据集\n",
    "\n",
    "在本章的剩余部分，我们将对之前章节中实现并预训练的 GPT 模型进行修改和分类微调。我们从下载并准备数据集开始，如图 6.4 所示。\n",
    "\n",
    "<img src=\"../Image/chapter6/figure6.4.png\" width=\"75%\" />\n",
    "\n",
    "为了提供一个直观实用的分类微调示例，我们将采用一个包含垃圾消息和非垃圾消息的文本消息数据集。\n",
    "\n",
    "注意，这里讨论的是通过手机发送的短信，而不是电子邮件。不过，相同的步骤也适用于电子邮件分类，感兴趣的读者可以在附录 B 的参考部分找到邮件垃圾分类数据集的链接。\n",
    "\n",
    "首先，通过以下代码下载数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "857c6927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/sms_spam_collection/SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.1 Downloading and unzipping the dataset\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"../data/sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
    "    if data_file_path.exists():\n",
    "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
    "        return\n",
    "\n",
    "    # 下载数据集\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        with open(zip_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "\n",
    "    # 解压数据集\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
    "    # 为解压的数据集文件设置.csv文件扩展名\n",
    "    os.rename(original_file_path, data_file_path)\n",
    "    print(f\"File downloaded and saved as {data_file_path}\")\n",
    "\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96695236",
   "metadata": {},
   "source": [
    "执行完上述代码后，数据集被保存为制表符分隔的文本文件“SMSSpamCollection.tsv”，位于“sms_spam_collection”文件夹中。我们可以将其加载到 pandas DataFrame 中，方法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0aff9177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                               Text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad98f42",
   "metadata": {},
   "source": [
    "保存的数据集如图 6.5 所示：\n",
    "\n",
    "<img src=\"../Image/chapter6/figure6.5.png\" width=\"75%\" />\n",
    "\n",
    "我们来看一下数据集中类别标签的分布情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83926c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d7675",
   "metadata": {},
   "source": [
    "执行上述代码后，我们发现数据集中‘ham’（正常短信）比‘spam’（垃圾短信）出现频率更高。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defeec4e",
   "metadata": {},
   "source": [
    "为了简化起见，同时也因为我们倾向于使用小数据集进行教学（这便于更快地微调 LLM），我们选择对数据集进行下采样，每个类别保留 747 个样本。尽管处理类别不平衡的方法有多种，但这超出了本书关于 LLM 的讨论范围。读者若有兴趣探索处理不平衡数据的方法，可以参考附录 B 的参考部分。\n",
    "\n",
    "我们可以通过以下代码对数据集进行下采样，以创建一个平衡的数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d22f04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "ham     747\n",
      "spam    747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Listing 6.2 Creating a balanced dataset\n",
    "def create_balanced_dataset(df):\n",
    "    # 统计垃圾短信的实例数量\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    # 随机抽取正常邮件实例，使其数量与垃圾短信实例相同。\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    # 将正常短信子集与垃圾短信合并\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "    return balanced_df\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "print(balanced_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9ea5a7",
   "metadata": {},
   "source": [
    "在执行了以上代码以平衡数据集后，我们可以看到现在垃圾短信和正常短信的数量相等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe1bb42",
   "metadata": {},
   "source": [
    "接下来，我们将字符串类别标签 \"ham\" 和 \"spam\" 分别转换为整数类别标签 0 和 1："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d18bf98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c071a3",
   "metadata": {},
   "source": [
    "这个过程类似于将文本转换为 token ID，但与使用包含 5 万多个词的 GPT 词汇表不同，这里我们仅处理两个 token ID：0 和 1。\n",
    "\n",
    "我们还需创建一个`random_split`函数，将数据集划分为三部分：70%用于训练，10%用于验证，20%用于测试。这些比例是机器学习中用于训练、调整和评估模型的常见划分比例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ed13f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.3 Splitting the dataset\n",
    "def random_split(df, train_frac, validation_frac):\n",
    "    # 随机打乱数据集\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "    # 计算数据分割的索引\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "\n",
    "    # 分割数据集\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "\n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "# 测试集默认大小为 0.2（即剩余部分）\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b6c219",
   "metadata": {},
   "source": [
    "此外，我们将数据集保存为 CSV 文件，以便后续复用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf9f19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"../data/sms_spam_collection/train.csv\", index=None)\n",
    "validation_df.to_csv(\"../data/sms_spam_collection/validation.csv\", index=None)\n",
    "test_df.to_csv(\"../data/sms_spam_collection/test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd13ec6",
   "metadata": {},
   "source": [
    "本节中，我们已经完成了数据集的下载、数据平衡处理，并将其划分为训练集和验证集。在接下来的部分中，我们将设置用于模型训练的 PyTorch 数据加载器。\n",
    "\n",
    "---\n",
    "\n",
    "## 6.3 创建数据加载器\n",
    "\n",
    "在本节中，我们将开发 PyTorch 数据加载器，其概念与第 2 章中实现的加载器类似。\n",
    "\n",
    "在第2章中，我们使用滑动窗口技术生成了大小一致的文本块，并将它们分组成批次，以提高模型训练的效率。每个文本块都作为一个独立的训练实例。\n",
    "\n",
    "然而，本章中我们使用的垃圾短信数据集包含长度不一的文本消息。为了像第 2 章中的文本块那样对这些消息进行批处理，我们有两种处理方式：\n",
    "\n",
    "1. 将所有消息截断至数据集或批次中最短消息的长度。\n",
    "2. 将所有消息填充到数据集或批次中最长消息的长度。\n",
    "\n",
    "方案一的计算成本较低，但如果较短的消息远小于平均长度或最长消息长度，可能会导致显著的信息损失，从而降低模型的性能。因此，我们选择方案二，以完整保留所有消息的内容。\n",
    "\n",
    "为实现方案二，我们需要将所有消息填充到与数据集中最长消息相同的长度，对所有较短的消息添加填充 token。为此，我们使用 `\"<|endoftext|>\"` 作为填充 token，正如第 2 章中所讨论的。\n",
    "\n",
    "在实现细节上，我们可以在编码后的文本消息中添加与 `\"<|endoftext|>\"` 对应的 token ID，而不是直接将字符串 `\"<|endoftext|>\"` 附加到每条文本消息后，如图 6.6 所示。\n",
    "\n",
    "<img src=\"../Image/chapter6/figure6.6.png\" width=\"75%\" />\n",
    "\n",
    "图 6.6 假定 50,256 是填充 token `<|endoftext|>` 的 token ID。我们可以通过使用 tiktoken 包中的 GPT-2 分词器对 `<|endoftext|>` 进行编码来进一步验证此 token ID 是否正确（该分词器在前几章中已使用过）:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91f7cef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4802995",
   "metadata": {},
   "source": [
    "执行以上代码，我们发现确实返回了 `[50256]`。\n",
    "\n",
    "接着，我们需要实例化数据加载器。但在此之前，我们首先需要实现一个 PyTorch Dataset，用于定义数据的加载和处理方式。\n",
    "\n",
    "为此，我们定义了`SpamDataset`类，实现了图 6.6 中展示的概念。该类负责处理多个关键任务：它识别训练数据集中最长的序列，对文本消息进行编码，并确保通过填充 token 将其他序列补齐到与最长序列相同的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e91c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.4 Setting up a Pytorch Dataset class\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "\n",
    "        # 对文本进行预分词\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "\n",
    "            # 若序列超过最大长度则进行截断\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "\n",
    "        # 将序列填充至最长序列长度\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809654cd",
   "metadata": {},
   "source": [
    "`SpamDataset`类从之前创建的 CSV 文件中加载数据，使用 tiktoken 库中的 GPT-2 分词器对文本进行分词，并支持将序列填充或截断为统一长度（由最长序列或预定义的最大长度决定）。这样可以确保每个输入张量大小一致，从而满足接下来数据加载器创建批量训练数据的需求："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1c8c5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"../data/sms_spam_collection/train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260dfae4",
   "metadata": {},
   "source": [
    "请注意，数据集的 `max_length` 属性中存储了最大序列长度。如果想要查看最长序列的 token 数量，可以使用以下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c205436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ddad75",
   "metadata": {},
   "source": [
    "代码输出了 120，表明最长的序列不超过 120 个 token，这也是文本消息的常见长度。值得注意的是，我们之前预训练的模型的上下文长度限制为 1,024 个 token，因此可以处理最长 1,024 个 token 的序列。如果数据集中包含更长的文本，可以在创建训练数据集时传入 `max_length=1024` 参数，以确保数据不会超出模型支持的输入（上下文）长度。\n",
    "\n",
    "接下来，我们将验证集和测试集的序列填充到与训练集中最长序列相同的长度。需要注意的是，如果验证集和测试集中的某些样本长度超过了训练集中最长样本的长度，会在先前定义的 `SpamDataset` 代码中通过 `encoded_text[:self.max_length]` 进行截断。这种截断是可选的；如果确保验证集和测试集中没有超过 1,024 个 token 的序列，也可以将 `max_length` 设置为 `None` 来避免截断。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22b4fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"../data/sms_spam_collection/validation.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"../data/sms_spam_collection/test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6415b6",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    ">\n",
    "> 练习6.1 扩展上下文长度\n",
    ">\n",
    "> 将输入补齐到模型支持的最大 token 数量，并观察其对预测性能的影响。\n",
    "\n",
    "将以上的数据集作为输入，我们就可以实例化数据加载器（可以回顾第 2 章中的操作）。然而，在本例中，目标表示的是类别标签，而非文本中的下一个 token。例如，选择批量大小为 8 时，每个批次包含 8 个长度为 120 的训练样本和相应的类别标签，如图 6.7 所示。\n",
    "\n",
    "<img src=\"../Image/chapter6/figure6.7.png\" width=\"75%\" />\n",
    "\n",
    "以下代码创建了训练集、验证集和测试集的数据加载器，以批量大小为 8 加载文本消息及其标签（如图 6.7 所示）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edd6bc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.5 Creating PyTorch data loaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 此设置可确保与大多数计算机兼容\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e90564",
   "metadata": {},
   "source": [
    "为了确保数据加载器正常工作并确实返回了预期大小的批次数据，我们可以遍历训练集数据加载器，并打印最后一个批次的张量维度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92703a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c2fb51",
   "metadata": {},
   "source": [
    "如上所示，输入批次包含 8 个训练样本，每个样本包含 120 个token。标签张量存储了对应 8 个训练样本的类别标签。\n",
    "\n",
    "最后，为了了解数据集的大小，可以打印每个数据集的批次数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8642fa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847247dd",
   "metadata": {},
   "source": [
    "本章的数据准备工作到此结束，接下来我们将初始化模型以准备进行微调。\n",
    "\n",
    "---\n",
    "\n",
    "## 6.4 使用预训练权重初始化模型\n",
    "\n",
    "在本节中，我们将准备用于垃圾短信分类微调的模型。首先，我们初始化上一章使用过的预训练模型，如图 6.8 所示。\n",
    "\n",
    "<img src=\"../Image/chapter6/figure6.8.png\" width=\"75%\" />\n",
    "\n",
    "现在我们通过复用第 5 章的配置，开始进行模型准备过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d97a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257, # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"drop_rate\": 0.0, # Dropout rate\n",
    "    \"qkv_bias\": True # Query-key-value bias\n",
    "}\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "assert train_dataset.max_length <= BASE_CONFIG[\"context_length\"], (\n",
    "    f\"Dataset length {train_dataset.max_length} exceeds model's context \"\n",
    "    f\"length {BASE_CONFIG['context_length']}. Reinitialize data sets with \"\n",
    "    f\"`max_length={BASE_CONFIG['context_length']}`\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f9e70d",
   "metadata": {},
   "source": [
    "接下来，我们从第 5 章下载的 `gpt_download.py` 文件中导入 `download_and_load_gpt2` 函数。同时，我们还可以复用第 5 章中的 `GPTModel` 类和 `load_weights_into_gpt` 函数，将下载的权重加载到 GPT 模型中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa074206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 6.6 Loading a pretrained GPT model\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()) + '/code/')\n",
    "\n",
    "from gpt_download import download_and_load_gpt2\n",
    "from chapter4 import GPTModel\n",
    "from chapter5 import load_weights_into_gpt\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f077c20d",
   "metadata": {},
   "source": [
    "在将模型权重加载到`GPTModel`后，我们使用前面章节的文本生成工具函数，确保模型能够生成连贯的文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dc38ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "from chapter4 import generate_text_simple\n",
    "from chapter5 import text_to_token_ids, token_ids_to_text\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2614e7",
   "metadata": {},
   "source": [
    "现在，在我们开始将模型微调为垃圾短信分类器之前，我们先来看看这个模型是否能通过给它提供指令来对垃圾短信进行分类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b03353b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n",
      "The following text 'spam'? Answer with 'yes' or 'no': 'You are a winner\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=23,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26772640",
   "metadata": {},
   "source": [
    "根据输出结果，可以看到模型还不具备遵循指令方面的能力。\n",
    "\n",
    "这是预料之中的，因为它仅经过了预训练，缺乏指令微调，我们将在下一章探讨这个问题。\n",
    "\n",
    "下一节开始为模型的分类微调做准备。\n",
    "\n",
    "---\n",
    "\n",
    "## 6.5 添加分类头\n",
    "\n",
    "本节我们将修改预训练的模型，为分类任务的微调做准备。为此，我们需要替换原始输出层，原输出层将隐层表示映射到50,257个词汇的词汇表，而我们用一个较小的输出层将其映射到两个类别：0（‘非垃圾短信’）和1（‘垃圾短信’），如图6.9所示。\n",
    "\n",
    "<img src=\"../Image/chapter6/figure6.9.png\" width=\"75%\" />\n",
    "\n",
    "如图 6.9 所示，我们使用与前几章相同的模型，唯一的不同是替换了输出层。\n",
    "\n",
    "> [!NOTE]\n",
    ">\n",
    "> **输出层节点**\n",
    ">\n",
    "> 理论上，由于我们处理的是二分类任务，可以使用单个输出节点。然而，这需要修改损失函数，具体内容可以参见附录B的参考部分。因此，我们选择一个更通用的方法，即输出节点数与类别数相匹配。例如，对于一个三分类问题，如将新闻文章分类为“技术”、“体育”或“政治”，我们使用三个输出节点，以此类推。\n",
    "\n",
    "在我们尝试图 6.9 中展示的修改之前，先通过 `print(model)` 打印模型架构，结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7c971b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298d5d59",
   "metadata": {},
   "source": [
    "上图清晰展示了我们在第 4 章实现的架构：GPT 模型由嵌入层、12 个相同的 Transformer 模块（出于简洁考虑，只展示了最后一个模块）构成，接着是最终的 LayerNorm 层和输出层（out_head）。\n",
    "\n",
    "接下来我们将用一个新的输出层替换原始输出层（见图 6.9），并对其进行微调。\n",
    "\n",
    "> [!NOTE]\n",
    ">\n",
    "> **微调部分层与全部层的对比**\n",
    ">\n",
    "> 由于我们从预训练模型开始，并不需要对所有模型层进行微调。这是因为，在基于神经网络的语言模型中，低层通常捕捉到的是基本的语言结构和语义，这些特征适用于多种任务和数据集。因此，只微调最后几层（接近输出层），它们更专注于细致的语言模式和任务特定的特征，通常就足够使模型适应新任务。此外，微调较少的层在计算上也更加高效。对于有兴趣的读者，可以在附录B的参考部分找到更多关于微调哪些层的详细信息，包括相关实验。\n",
    "\n",
    "为了让模型准备好进行分类微调，我们首先通过将所有层设为不可训练来冻结模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edd7ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad9a4ff",
   "metadata": {},
   "source": [
    "接着，按照图 6.9 所示，我们替换掉输出层（model.out_head），该层原本将层输入映射到 50,257 维空间（即词汇表大小）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e292797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.7 Adding a classification layer\n",
    "torch.manual_seed(123)\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(\n",
    "    in_features=BASE_CONFIG[\"emb_dim\"],\n",
    "    out_features=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20455175",
   "metadata": {},
   "source": [
    "请注意，在上述代码中我们使用了 `BASE_CONFIG[\"emb_dim\"]`，在 `gpt2-small (124M)` 模型中它的值为 768，这样可以让后续代码更加通用，便于适配更大的 GPT-2 模型变体。\n",
    "\n",
    "这个新的输出层 `model.out_head` 的 `requires_grad` 属性默认为 `True`，意味着它是模型训练过程中唯一会被更新的层。\n",
    "\n",
    "从技术上讲，训练我们刚添加的输出层已经足够。然而，通过实验我发现，微调更多层能够显著提升微调后模型的预测性能（更多细节请参考附录 C 中的参考文献）。\n",
    "\n",
    "此外，我们还需将最后一个 Transformer 模块以及连接该模块和输出层的 LayerNorm 模块配置为可训练，如图6.10所示。\n",
    "\n",
    "<img src=\"../Image/chapter6/figure6.10.png\" width=\"75%\" />\n",
    "\n",
    "为了让最终的 LayerNorm 和最后一个 Transformer 模块参与训练（如图 6.10 所示），我们将它们的 `requires_grad` 设置为 `True：`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98a88e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.final_norm.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac798c1",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    ">\n",
    "> **微调整个模型**\n",
    ">\n",
    "> 与仅微调最后一个 Transformer 模块相比，可以微调整个模型并评估其对预测性能的影响。\n",
    "\n",
    "尽管我们增加了一个新的输出层，并标记了某些层为可训练或不可训练，我们仍然可以像前几章那样使用这个模型。例如，我们可以像以前一样向模型输入一个示例文本。考虑以下示例文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbb4a466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([[5211,  345,  423,  640]])\n",
      "Inputs dimensions: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0)\n",
    "print(\"Inputs:\", inputs)\n",
    "print(\"Inputs dimensions:\", inputs.shape) # shape: (batch_size, num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106c8b27",
   "metadata": {},
   "source": [
    "从输出结果可以看出，前面的代码将输入编码成了一个包含 4 个输入 token 的张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ab1fdd",
   "metadata": {},
   "source": [
    "接着，我们将编码后的 token ID 直接传入模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebe64505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:\n",
      " tensor([[[-1.5854,  0.9904],\n",
      "         [-3.7235,  7.4548],\n",
      "         [-2.2661,  6.6049],\n",
      "         [-3.5983,  3.9902]]])\n",
      "Outputs dimensions: torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "    print(\"Outputs:\\n\", outputs)\n",
    "    print(\"Outputs dimensions:\", outputs.shape)  # shape: (batch_size, num_tokens, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc6a41c",
   "metadata": {},
   "source": [
    "在第 4 章和第 5 章中，相似的输入会生成形状为 [1, 4, 50257] 的输出张量，其中 50,257 表示词汇表大小。与前几章相同，输出张量的行数对应输入的 token 数量（在这里是 4 个）。不过，由于替换了模型的输出层，现在每个输出的嵌入维度（即列数）从 50,257 缩减为 2。\n",
    "\n",
    "请注意，我们希望微调该模型，使其能够输出一个分类标签，用于判断输入是否为垃圾短信。为实现这一点，我们不需要微调所有 4 行输出，只需聚焦于单个输出  token。具体来说，我们将重点关注最后一行对应的输出 token，如图 6.11 所示。\n",
    "\n",
    "<img src=\"../Image/chapter6/figure6.11.png\" width=\"75%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52e5c433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n"
     ]
    }
   ],
   "source": [
    "# To extract the last output token, illustrated in figure 6.11, from the output tensor, we use the following code:\n",
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c58798",
   "metadata": {},
   "source": [
    "接下来，我们将重点讨论如何将这些值转换为类别标签预测。但在此之前，我们需要理解，为什么我们特别关注最后一个输出的token，而不是第一个、第二个或第三个输出token。\n",
    "\n",
    "在第 3 章中，我们探讨了注意力机制，该机制在每个输入 token 与其他所有输入 token 之间建立关系。随后，我们引入了因果注意力掩码的概念，这在 GPT 类模型中被广泛使用。这种掩码限制每个 token 的关注范围，使其只能关注当前位置及之前的内容，从而确保每个 token 只能受到自身及前面 token 的影响，如图 6.12 所示。\n",
    "\n",
    "<img src=\"../Image/chapter6/figure6.12.png\" width=\"75%\" />\n",
    "\n",
    "在图 6.12 所示的因果注意力掩码设置中，序列中的最后一个 token 聚合了所有前面 token 的信息。因此，在垃圾短信分类任务的微调过程中，我们会重点关注这个最后的 token。\n",
    "\n",
    "在修改模型后，接下来将详细介绍如何将最后一个 token 转换为分类标签预测，并计算模型的初始预测准确率。之后，我们将在后续部分对模型进行垃圾短信分类任务的微调。\n",
    "\n",
    "> [!NOTE]\n",
    ">\n",
    "> **第一个 token 与最后一个 token 的微调对比**\n",
    ">\n",
    "> 尝试微调第一个输出 token，而不是最后一个输出 token，并在后续章节的模型微调实验中观察预测性能的变化。\n",
    "\n",
    "---\n",
    "\n",
    "## 6.6 计算分类损失和准确率\n",
    "\n",
    "本章到目前为止，我们已完成了数据集准备、预训练模型的加载，以及对模型进行分类微调的修改。在微调正式开始前，还剩下一小部分工作：实现微调过程中使用的模型评估函数（如图 6.13 所示）。我们将在本节完成这一部分。\n",
    "\n",
    "<img src=\"../Image/chapter6/figure6.13.png\" width=\"75%\" />\n",
    "\n",
    "在实现评估工具之前，我们先简单讨论一下如何将模型输出转换为类别标签预测。\n",
    "\n",
    "在上一章中，我们通过 softmax 函数将 50,257 个输出转换为概率分布，然后通过 argmax 函数返回概率最高的位置，从而得到 LLM 生成的下一个 token 的 token ID。本章中，我们采用相同的方法来计算模型对于给定输入的预测结果是‘垃圾短信’还是‘正常短信’。唯一的区别是，这次的输出维度是 2，而不是 50,257 维。\n",
    "\n",
    "模型对每个输入文本的最后一个 token 生成的输出被转换为概率得分。然后，通过查找概率得分中最高值的位置来确定对应的分类标签。请注意，由于模型尚未经过训练，目前对垃圾短信标签的预测是不准确的。\n",
    "\n",
    "<img src=\"../Image/chapter6/figure6.14.png\" width=\"75%\" />\n",
    "\n",
    "为了通过具体示例来说明图 6.14，我们来看一下前一节代码示例中的最后一个输出 token："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86fab7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Last output token:\", outputs[:, -1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c2a422",
   "metadata": {},
   "source": [
    "我们可以通过以下代码获取分类标签："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a71fbf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "label = torch.argmax(probas)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0e26ae",
   "metadata": {},
   "source": [
    "在这种情况下，代码返回 1，表示模型预测输入文本为‘垃圾短信’。这里使用 Softmax 函数是可选的，因为最大的输出值已经对应最高的概率分数（参见第 5 章）。因此，我们可以省略 Softmax 函数，简化代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d689a290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label: 1\n"
     ]
    }
   ],
   "source": [
    "logits = outputs[:, -1, :]\n",
    "label = torch.argmax(logits)\n",
    "print(\"Class label:\", label.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734116e4",
   "metadata": {},
   "source": [
    "这个概念可以用来计算分类准确率，它衡量的是数据集上正确预测的比例。\n",
    "\n",
    "为了计算分类准确率，我们对数据集中的所有样本进行 argmax 预测，并通过定义一个 `calc_accuracy_loader` 函数来计算预测正确的比例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3ef033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.8 Calculating the classification accuracy\n",
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # 最后一个输出 token 的 logits 值\n",
    "                logits = model(input_batch)[:, -1, :]\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebab761",
   "metadata": {},
   "source": [
    "我们可以使用这个函数来估算多个数据集上的分类准确率，为提高效率，这里基于 10 个批次的结果进行估算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a42a80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320a002a",
   "metadata": {},
   "source": [
    "通过设置`device`属性，如果检测到支持 Nvidia CUDA 的 GPU，模型会自动在 GPU 上运行，否则会在 CPU 上运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc54a808",
   "metadata": {},
   "source": [
    "可以看到，当前模型的预测准确率接近随机预测（在本例中为 50%）。为了提高预测准确率，我们需要对模型进行微调。\n",
    "\n",
    "在微调模型之前，我们需要定义损失函数，以便在训练过程中对其进行优化。我们的目标是最大化模型的垃圾短信分类准确率，因此代码输出应为正确的类别标签：0 表示正常短信，1 表示垃圾短信。\n",
    "\n",
    "然而，由于分类准确率不是一个可微分的函数，因此我们使用交叉熵损失作为替代来优化准确率。这里所说的交叉熵损失与第 5 章讨论的一致。\n",
    "\n",
    "因此，`calc_loss_batch` 函数与第五章中的版本基本相同，唯一的调整是：我们只优化最后一个 token（`model(input_batch)[:, -1, :]`），而不是整个序列中的所有 token（`model(input_batch)`）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2c98194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)[:, -1, :] # Logits of last output token\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb93f912",
   "metadata": {},
   "source": [
    "我们使用 `calc_loss_batch` 函数来计算从前面定义的数据加载器获取的单个批次的损失。为了计算数据加载器中所有批次的损失，我们定义了 `calc_loss_loader` 函数，其功能与第五章中的描述相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c123e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.9 Calculating the classification loss\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # 确保批次数不超过数据加载器中的总批次数\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "# Similar to calculating the training accuracy, we now compute the initial loss for each data set:\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1db49f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.453\n",
      "Validation loss: 2.583\n",
      "Test loss: 2.322\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc6c3ff",
   "metadata": {},
   "source": [
    "在下一节，我们将实现一个训练函数来微调模型，实现最小化训练集损失。最小化训练集损失将有助于提高分类准确性，这是我们的总体目标。\n",
    "\n",
    "---\n",
    "\n",
    "## 6.7 使用监督数据对模型进行微调\n",
    "\n",
    "在本节中，我们定义并使用训练函数，对预训练的 LLM 进行微调，以提升其垃圾短信分类的准确率。训练循环的整体结构与第 5 章中的相同（详见图 6.15），唯一的区别在于，这里计算的是分类准确率，而不是通过生成文本来评估模型。\n",
    "\n",
    "<img src=\"../Image/chapter6/figure6.15.png\" width=\"75%\" />\n",
    "\n",
    "可以看到，图 6.15 中所示的训练函数逻辑，与第 5 章中用于模型预训练的 `train_model_simple` 函数非常相似。\n",
    "\n",
    "唯一的两个区别在于：现在记录的是训练样本数量（examples_seen），而不是 token 数量；并且在每个 epoch 后计算准确率，而不再打印示例文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04cd4dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.10 Finetuning the model to classify spam\n",
    "def train_classifier_simple(model, train_loader, val_loader, optimizer, device,\n",
    "num_epochs, eval_freq, eval_iter, tokenizer):\n",
    "    # Initialize lists to track losses and examples seen\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            examples_seen += input_batch.shape[0]\n",
    "            global_step += 1\n",
    "\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        train_accuracy = calc_accuracy_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_accuracy = calc_accuracy_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2cfa9a",
   "metadata": {},
   "source": [
    "以上 `train_classifier_simple` 中使用的 `evaluate_model` 函数与我们在第 5 章中使用的函数相同："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60ff85ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6662885",
   "metadata": {},
   "source": [
    "接下来，我们初始化优化器，设置训练轮数，并通过 `train_classifier_simple` 函数启动训练。关于训练轮数的选择将在评估结果后讨论。在 M3 MacBook Air 上训练大约需要 6 分钟，而在 V100 或 A100 GPU 上则不到半分钟："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42eee62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.153, Val loss 2.392\n",
      "Ep 1 (Step 000050): Train loss 0.617, Val loss 0.637\n",
      "Ep 1 (Step 000100): Train loss 0.523, Val loss 0.557\n",
      "Training accuracy: 70.00% | Validation accuracy: 72.50%\n",
      "Ep 2 (Step 000150): Train loss 0.561, Val loss 0.489\n",
      "Ep 2 (Step 000200): Train loss 0.419, Val loss 0.397\n",
      "Ep 2 (Step 000250): Train loss 0.409, Val loss 0.353\n",
      "Training accuracy: 82.50% | Validation accuracy: 85.00%\n",
      "Ep 3 (Step 000300): Train loss 0.333, Val loss 0.320\n",
      "Ep 3 (Step 000350): Train loss 0.340, Val loss 0.306\n",
      "Training accuracy: 90.00% | Validation accuracy: 90.00%\n",
      "Ep 4 (Step 000400): Train loss 0.136, Val loss 0.200\n",
      "Ep 4 (Step 000450): Train loss 0.153, Val loss 0.132\n",
      "Ep 4 (Step 000500): Train loss 0.222, Val loss 0.137\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Ep 5 (Step 000550): Train loss 0.207, Val loss 0.143\n",
      "Ep 5 (Step 000600): Train loss 0.083, Val loss 0.074\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Training completed in 4.55 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "num_epochs = 5\n",
    "\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c6e5c2",
   "metadata": {},
   "source": [
    "类似于第 5 章的做法，我们使用 matplotlib 绘制训练集和验证集的损失函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "82cb0ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.11 Plotting the classification loss\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # 绘制训练轮次与训练和验证损失的变化图\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "\n",
    "    # 创建一个新的 x 轴，用于显示已处理样本数\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(examples_seen, train_values, alpha=0) # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6b0c785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d8f006",
   "metadata": {},
   "source": [
    "图6.16展示了最终的损失曲线。\n",
    "\n",
    "<img src=\"../Image/chapter6/figure6.16.png\" width=\"75%\" />\n",
    "\n",
    "从图 6.16 中陡峭的下降曲线可以看出，模型在训练数据上的学习效果很好，且没有明显的过拟合迹象，训练集和验证集的损失值几乎没有差距。\n",
    "\n",
    "> [!NOTE]\n",
    ">\n",
    "> **选择训练轮数**\n",
    ">\n",
    "> 在训练开始时，我们将 epoch 数量设置为 5。epoch 的具体数量取决于数据集和任务的难度，并没有通用的解决方案或推荐值。5 个 epoch 通常是一个合适的起点。如果在前几个 epoch 后模型出现过拟合迹象（如图 6.16 所示的损失曲线显示验证损失上升），我们可能需要减少 epoch 数量。相反，如果趋势线显示验证损失随着训练仍有下降空间，我们则应增加 epoch 数量。在本例中，5 个 epoch 是合理的选择，因为没有早期过拟合的迹象，且验证损失接近 0。\n",
    "\n",
    "接下来，继续使用 plot_values 函数绘制分类准确率的图表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55581867",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")\n",
    "The resulting accuracy graphs are shown in figure 6.17."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f460e86",
   "metadata": {},
   "source": [
    "<img src=\"../Image/chapter6/figure6.17.png\" width=\"75%\" />\n",
    "\n",
    "从图 6.17 的准确率曲线可以看出，模型在第 4 到 5 个训练周期后，训练和验证准确率均达到了较高水平。\n",
    "\n",
    "需要注意的是，我们之前在使用 `train_classifier_simple` 函数时将 `eval_iter` 设置为 5，这意味着我们的训练和验证性能估计仅基于 5 个批次，目的是为了提高训练效率。\n",
    "\n",
    "现在，我们将通过运行以下代码，计算整个数据集在训练集、验证集和测试集上的性能指标，这次不需要定义 `eval_iter` 值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "411d7c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 97.21%\n",
      "Validation accuracy: 97.32%\n",
      "Test accuracy: 95.67%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f3efcc",
   "metadata": {},
   "source": [
    "可以看到，训练集和测试集的表现几乎相同。\n",
    "\n",
    "训练集和测试集准确率之间的轻微差异表明训练数据的过拟合程度较低。通常，验证集的准确率会略高于测试集的准确率，这是因为模型开发过程中通常会通过调整超参数来优化验证集上的表现，而这种优化未必能有效地泛化到测试集上。\n",
    "\n",
    "这种情况很常见，但可以通过调整模型设置来减小这种差距，比如增加 dropout 率（`drop_rate`）或优化器配置中的权重衰减（`weight_decay`）参数。\n",
    "\n",
    "---\n",
    "\n",
    "## 6.8 将 LLM 用于垃圾短信分类\n",
    "\n",
    "在前几节对模型进行微调和评估后，我们现在进入本章的最后阶段（见图 6.18）：使用模型进行垃圾短信分类。\n",
    "\n",
    "<img src=\"../Image/chapter6/figure6.18.png\" width=\"75%\" />\n",
    "\n",
    "最后，我们将使用微调后的基于 GPT 的垃圾短信分类模型。以下的 `classify_review` 函数遵循了与本章之前实现的 `SpamDataset` 类似的数据预处理步骤。函数先将文本处理为 token ID，然后使用模型预测一个整数类别标签（与 6.6 节中的实现类似），并返回对应的类别名称："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a1e5d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 6.12 Using the model to classify new texts\n",
    "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    model.eval()\n",
    "\n",
    "    # 将文本转换为 token ID\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_context_length = model.pos_emb.weight.shape[1]\n",
    "\n",
    "    # 截断过长序列\n",
    "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
    "\n",
    "    # 填充序列至最长长度\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "\n",
    "    # 增加批次维度\n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)[:, -1, :]\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe51b25",
   "metadata": {},
   "source": [
    "我们来试试用示例文本测试 classify_review 函数的效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7fe15b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "text_1 = (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    ")\n",
    "\n",
    "print(classify_review(\n",
    "    text_1, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372f8ffc",
   "metadata": {},
   "source": [
    "训练得到的模型正确预测了‘spam’。接下来，让我们尝试另一个示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2993b00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not spam\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"Hey, just wanted to check if we're still on\"\n",
    "    \" for dinner tonight? Let me know!\"\n",
    ")\n",
    "print(classify_review(\n",
    "    text_2, model, tokenizer, device, max_length=train_dataset.max_length\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061f35d0",
   "metadata": {},
   "source": [
    "这个实例也一样，模型做出了正确预测并返回了‘非垃圾短信’标签。\n",
    "\n",
    "最后，为了方便后续重复使用模型，避免再次训练，我们可以使用上一章介绍的 `torch.save` 方法来保存模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4173529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"review_classifier.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af074938",
   "metadata": {},
   "source": [
    "保存后，可以按如下方式加载模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ccf3706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_state_dict = torch.load(\"review_classifier.pth\")\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3c81a0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6.9 本章摘要\n",
    "\n",
    "+ 微调 LLM 有不同的策略，包括分类微调（本章）和指令微调（下一章）。\n",
    "+ 分类微调是指将 LLM 的输出层替换为一个小型的分类层。\n",
    "+ 在将文本消息分类为‘垃圾短信’或‘非垃圾短信’的任务中，新的分类层只需要 2 个输出节点；而在之前的章节中，输出节点的数量等于词汇表中的唯一 token 数量，即 50,256。\n",
    "+ 分类微调任务不是像预训练那样预测下一个词，而是训练模型输出正确的类别标签，例如‘垃圾短信’或‘非垃圾短信’。\n",
    "+ 在微调阶段，模型的输入是转换为 token ID 的文本，这与预训练阶段类似。\n",
    "+ 在微调 LLM 之前，我们会加载预训练模型作为基础。\n",
    "+ 评估分类模型需要计算分类准确率，即正确预测的比例。\n",
    "+ 微调分类模型时使用的交叉熵损失函数与预训练 LLM 时相同。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
